{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import ml_utils.profile as profile\n",
    "\n",
    "class qlora_forward(profile.BenchmarkCandidate):\n",
    "    def __init__(self, batch_size, hidden_size, rank):\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rank = rank\n",
    "        self.X = torch.randn((batch_size, hidden_size), device='cuda', dtype=torch.bfloat16)\n",
    "        self.W = torch.randn((hidden_size, hidden_size), device='cuda', dtype=torch.bfloat16)\n",
    "        self.U = torch.randn((hidden_size, rank), device='cuda', dtype=torch.bfloat16)\n",
    "        self.V = torch.randn((rank, hidden_size), device='cuda', dtype=torch.bfloat16)\n",
    "\n",
    "    def benchmark_content(self):\n",
    "        XW = torch.matmul(self.X, self.W)\n",
    "        XU = torch.matmul(self.X, self.U)\n",
    "        XUV = torch.matmul(XU, self.V)\n",
    "        return XW + XUV\n",
    "\n",
    "class qlora_low_rank_part(profile.BenchmarkCandidate):\n",
    "    def __init__(self, batch_size, hidden_size, rank):\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rank = rank\n",
    "        self.X = torch.randn((batch_size, hidden_size), device='cuda', dtype=torch.bfloat16)\n",
    "        self.W = torch.randn((hidden_size, hidden_size), device='cuda', dtype=torch.bfloat16)\n",
    "        self.U = torch.randn((hidden_size, rank), device='cuda', dtype=torch.bfloat16)\n",
    "        self.V = torch.randn((rank, hidden_size), device='cuda', dtype=torch.bfloat16)\n",
    "\n",
    "    def benchmark_content(self):\n",
    "        XU = torch.matmul(self.X, self.U)\n",
    "        XUV = torch.matmul(XU, self.V)\n",
    "        return XUV\n",
    "\n",
    "class qlora_dense_part(profile.BenchmarkCandidate):\n",
    "    def __init__(self, batch_size, hidden_size, rank):\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rank = rank\n",
    "        self.X = torch.randn((batch_size, hidden_size), device='cuda', dtype=torch.bfloat16)\n",
    "        self.W = torch.randn((hidden_size, hidden_size), device='cuda', dtype=torch.bfloat16)\n",
    "        self.U = torch.randn((hidden_size, rank), device='cuda', dtype=torch.bfloat16)\n",
    "        self.V = torch.randn((rank, hidden_size), device='cuda', dtype=torch.bfloat16)\n",
    "\n",
    "    def benchmark_content(self):\n",
    "        XW = torch.matmul(self.X, self.W)\n",
    "        return XW\n",
    "\n",
    "class qlora_addition_part(profile.BenchmarkCandidate):\n",
    "    def __init__(self, batch_size, hidden_size, rank):\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rank = rank\n",
    "        self.X = torch.randn((batch_size, hidden_size), device='cuda', dtype=torch.bfloat16)\n",
    "        self.W = torch.randn((hidden_size, hidden_size), device='cuda', dtype=torch.bfloat16)\n",
    "        self.U = torch.randn((hidden_size, rank), device='cuda', dtype=torch.bfloat16)\n",
    "        self.V = torch.randn((rank, hidden_size), device='cuda', dtype=torch.bfloat16)\n",
    "        self.XW = torch.matmul(self.X, self.W)\n",
    "        self.XU = torch.matmul(self.X, self.U)\n",
    "        self.XUV = torch.matmul(self.XU, self.V)\n",
    "\n",
    "    def benchmark_content(self):\n",
    "        return self.XW + self.XUV\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>name</th>\n",
       "      <th>qlora_addition_part</th>\n",
       "      <th>qlora_dense_part</th>\n",
       "      <th>qlora_forward</th>\n",
       "      <th>qlora_low_rank_part</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>batch_size</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>0.010240</td>\n",
       "      <td>0.027648</td>\n",
       "      <td>0.057344</td>\n",
       "      <td>0.024576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2048</th>\n",
       "      <td>0.017408</td>\n",
       "      <td>0.034816</td>\n",
       "      <td>0.062464</td>\n",
       "      <td>0.019456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3072</th>\n",
       "      <td>0.024576</td>\n",
       "      <td>0.036864</td>\n",
       "      <td>0.078048</td>\n",
       "      <td>0.023552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4096</th>\n",
       "      <td>0.030720</td>\n",
       "      <td>0.059392</td>\n",
       "      <td>0.114688</td>\n",
       "      <td>0.028672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5120</th>\n",
       "      <td>0.036864</td>\n",
       "      <td>0.054272</td>\n",
       "      <td>0.121856</td>\n",
       "      <td>0.033792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6144</th>\n",
       "      <td>0.044032</td>\n",
       "      <td>0.069632</td>\n",
       "      <td>0.148480</td>\n",
       "      <td>0.038912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7168</th>\n",
       "      <td>0.050176</td>\n",
       "      <td>0.071680</td>\n",
       "      <td>0.160768</td>\n",
       "      <td>0.041984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8192</th>\n",
       "      <td>0.056320</td>\n",
       "      <td>0.086080</td>\n",
       "      <td>0.187392</td>\n",
       "      <td>0.047104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9216</th>\n",
       "      <td>0.063488</td>\n",
       "      <td>0.103232</td>\n",
       "      <td>0.215040</td>\n",
       "      <td>0.052224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10240</th>\n",
       "      <td>0.070656</td>\n",
       "      <td>0.104416</td>\n",
       "      <td>0.227328</td>\n",
       "      <td>0.056320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11264</th>\n",
       "      <td>0.076800</td>\n",
       "      <td>0.119808</td>\n",
       "      <td>0.253952</td>\n",
       "      <td>0.061440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12288</th>\n",
       "      <td>0.082944</td>\n",
       "      <td>0.121856</td>\n",
       "      <td>0.267264</td>\n",
       "      <td>0.065536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13312</th>\n",
       "      <td>0.090112</td>\n",
       "      <td>0.138176</td>\n",
       "      <td>0.293888</td>\n",
       "      <td>0.069632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14336</th>\n",
       "      <td>0.096256</td>\n",
       "      <td>0.142336</td>\n",
       "      <td>0.309248</td>\n",
       "      <td>0.074752</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "name        qlora_addition_part  qlora_dense_part  qlora_forward  \\\n",
       "batch_size                                                         \n",
       "1024                   0.010240          0.027648       0.057344   \n",
       "2048                   0.017408          0.034816       0.062464   \n",
       "3072                   0.024576          0.036864       0.078048   \n",
       "4096                   0.030720          0.059392       0.114688   \n",
       "5120                   0.036864          0.054272       0.121856   \n",
       "6144                   0.044032          0.069632       0.148480   \n",
       "7168                   0.050176          0.071680       0.160768   \n",
       "8192                   0.056320          0.086080       0.187392   \n",
       "9216                   0.063488          0.103232       0.215040   \n",
       "10240                  0.070656          0.104416       0.227328   \n",
       "11264                  0.076800          0.119808       0.253952   \n",
       "12288                  0.082944          0.121856       0.267264   \n",
       "13312                  0.090112          0.138176       0.293888   \n",
       "14336                  0.096256          0.142336       0.309248   \n",
       "\n",
       "name        qlora_low_rank_part  \n",
       "batch_size                       \n",
       "1024                   0.024576  \n",
       "2048                   0.019456  \n",
       "3072                   0.023552  \n",
       "4096                   0.028672  \n",
       "5120                   0.033792  \n",
       "6144                   0.038912  \n",
       "7168                   0.041984  \n",
       "8192                   0.047104  \n",
       "9216                   0.052224  \n",
       "10240                  0.056320  \n",
       "11264                  0.061440  \n",
       "12288                  0.065536  \n",
       "13312                  0.069632  \n",
       "14336                  0.074752  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_grid = {\n",
    "    \"batch_size\": [1024 * i for i in range(1, 15)],\n",
    "    \"hidden_size\": [768],\n",
    "    \"rank\": [16]\n",
    "}\n",
    "\n",
    "bench_input_list = profile.params_grid_to_list(params_grid)\n",
    "\n",
    "batch_size_result = profile.benchmark(\n",
    "    {\n",
    "        \"qlora_forward\": qlora_forward,\n",
    "        \"qlora_low_rank_part\": qlora_low_rank_part,\n",
    "        \"qlora_dense_part\": qlora_dense_part,\n",
    "        \"qlora_addition_part\": qlora_addition_part\n",
    "    },\n",
    "    bench_input_list\n",
    ")\n",
    "\n",
    "pivoted = batch_size_result.pivot(index=\"batch_size\", columns=\"name\", values=\"ms\")\n",
    "\n",
    "pivoted\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for small models\n",
    "\n",
    "3 parts\n",
    "\n",
    "each takes around 1/3 of the total runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>name</th>\n",
       "      <th>qlora_addition_part</th>\n",
       "      <th>qlora_dense_part</th>\n",
       "      <th>qlora_forward</th>\n",
       "      <th>qlora_low_rank_part</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>batch_size</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>0.021504</td>\n",
       "      <td>0.082944</td>\n",
       "      <td>0.122880</td>\n",
       "      <td>0.023552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3072</th>\n",
       "      <td>0.056320</td>\n",
       "      <td>0.209920</td>\n",
       "      <td>0.311296</td>\n",
       "      <td>0.051200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5120</th>\n",
       "      <td>0.092160</td>\n",
       "      <td>0.334848</td>\n",
       "      <td>0.504640</td>\n",
       "      <td>0.072704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7168</th>\n",
       "      <td>0.126976</td>\n",
       "      <td>0.496640</td>\n",
       "      <td>0.712640</td>\n",
       "      <td>0.106496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9216</th>\n",
       "      <td>0.161792</td>\n",
       "      <td>0.608256</td>\n",
       "      <td>0.868352</td>\n",
       "      <td>0.131072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11264</th>\n",
       "      <td>0.197632</td>\n",
       "      <td>0.764928</td>\n",
       "      <td>1.091584</td>\n",
       "      <td>0.152576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13312</th>\n",
       "      <td>0.231424</td>\n",
       "      <td>0.863744</td>\n",
       "      <td>1.246208</td>\n",
       "      <td>0.182272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "name        qlora_addition_part  qlora_dense_part  qlora_forward  \\\n",
       "batch_size                                                         \n",
       "1024                   0.021504          0.082944       0.122880   \n",
       "3072                   0.056320          0.209920       0.311296   \n",
       "5120                   0.092160          0.334848       0.504640   \n",
       "7168                   0.126976          0.496640       0.712640   \n",
       "9216                   0.161792          0.608256       0.868352   \n",
       "11264                  0.197632          0.764928       1.091584   \n",
       "13312                  0.231424          0.863744       1.246208   \n",
       "\n",
       "name        qlora_low_rank_part  \n",
       "batch_size                       \n",
       "1024                   0.023552  \n",
       "3072                   0.051200  \n",
       "5120                   0.072704  \n",
       "7168                   0.106496  \n",
       "9216                   0.131072  \n",
       "11264                  0.152576  \n",
       "13312                  0.182272  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_grid = {\n",
    "    \"batch_size\": [1024 * i for i in range(1, 15, 2)],\n",
    "    \"hidden_size\": [2048],\n",
    "    \"rank\": [16]\n",
    "}\n",
    "\n",
    "bench_input_list = profile.params_grid_to_list(params_grid)\n",
    "\n",
    "batch_size_result = profile.benchmark(\n",
    "    {\n",
    "        \"qlora_forward\": qlora_forward,\n",
    "        \"qlora_low_rank_part\": qlora_low_rank_part,\n",
    "        \"qlora_dense_part\": qlora_dense_part,\n",
    "        \"qlora_addition_part\": qlora_addition_part\n",
    "    },\n",
    "    bench_input_list\n",
    ")\n",
    "\n",
    "pivoted = batch_size_result.pivot(index=\"batch_size\", columns=\"name\", values=\"ms\")\n",
    "\n",
    "pivoted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For middle sized models\n",
    "\n",
    "dense takes the most of the time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>name</th>\n",
       "      <th>qlora_addition_part</th>\n",
       "      <th>qlora_dense_part</th>\n",
       "      <th>qlora_forward</th>\n",
       "      <th>qlora_low_rank_part</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>batch_size</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>0.038912</td>\n",
       "      <td>0.296864</td>\n",
       "      <td>0.367616</td>\n",
       "      <td>0.036864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3072</th>\n",
       "      <td>0.109568</td>\n",
       "      <td>0.899072</td>\n",
       "      <td>1.061888</td>\n",
       "      <td>0.082944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5120</th>\n",
       "      <td>0.179200</td>\n",
       "      <td>1.490944</td>\n",
       "      <td>1.735680</td>\n",
       "      <td>0.125952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7168</th>\n",
       "      <td>0.248832</td>\n",
       "      <td>2.153984</td>\n",
       "      <td>2.470912</td>\n",
       "      <td>0.173056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9216</th>\n",
       "      <td>0.318464</td>\n",
       "      <td>2.742272</td>\n",
       "      <td>3.168256</td>\n",
       "      <td>0.221184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11264</th>\n",
       "      <td>0.389120</td>\n",
       "      <td>3.350528</td>\n",
       "      <td>3.878912</td>\n",
       "      <td>0.269312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13312</th>\n",
       "      <td>0.458752</td>\n",
       "      <td>3.405824</td>\n",
       "      <td>4.115456</td>\n",
       "      <td>0.311296</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "name        qlora_addition_part  qlora_dense_part  qlora_forward  \\\n",
       "batch_size                                                         \n",
       "1024                   0.038912          0.296864       0.367616   \n",
       "3072                   0.109568          0.899072       1.061888   \n",
       "5120                   0.179200          1.490944       1.735680   \n",
       "7168                   0.248832          2.153984       2.470912   \n",
       "9216                   0.318464          2.742272       3.168256   \n",
       "11264                  0.389120          3.350528       3.878912   \n",
       "13312                  0.458752          3.405824       4.115456   \n",
       "\n",
       "name        qlora_low_rank_part  \n",
       "batch_size                       \n",
       "1024                   0.036864  \n",
       "3072                   0.082944  \n",
       "5120                   0.125952  \n",
       "7168                   0.173056  \n",
       "9216                   0.221184  \n",
       "11264                  0.269312  \n",
       "13312                  0.311296  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_grid = {\n",
    "    \"batch_size\": [1024 * i for i in range(1, 15, 2)],\n",
    "    \"hidden_size\": [4096],\n",
    "    \"rank\": [16]\n",
    "}\n",
    "\n",
    "bench_input_list = profile.params_grid_to_list(params_grid)\n",
    "\n",
    "batch_size_result = profile.benchmark(\n",
    "    {\n",
    "        \"qlora_forward\": qlora_forward,\n",
    "        \"qlora_low_rank_part\": qlora_low_rank_part,\n",
    "        \"qlora_dense_part\": qlora_dense_part,\n",
    "        \"qlora_addition_part\": qlora_addition_part\n",
    "    },\n",
    "    bench_input_list\n",
    ")\n",
    "\n",
    "pivoted = batch_size_result.pivot(index=\"batch_size\", columns=\"name\", values=\"ms\")\n",
    "\n",
    "pivoted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large models, dense taks the most"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increase the lora rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>name</th>\n",
       "      <th>qlora_addition_part</th>\n",
       "      <th>qlora_dense_part</th>\n",
       "      <th>qlora_forward</th>\n",
       "      <th>qlora_low_rank_part</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>batch_size</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>0.010240</td>\n",
       "      <td>0.027648</td>\n",
       "      <td>0.052224</td>\n",
       "      <td>0.018432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3072</th>\n",
       "      <td>0.023552</td>\n",
       "      <td>0.036864</td>\n",
       "      <td>0.081920</td>\n",
       "      <td>0.025600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5120</th>\n",
       "      <td>0.036864</td>\n",
       "      <td>0.054272</td>\n",
       "      <td>0.126976</td>\n",
       "      <td>0.038912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7168</th>\n",
       "      <td>0.050176</td>\n",
       "      <td>0.070656</td>\n",
       "      <td>0.169984</td>\n",
       "      <td>0.051200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9216</th>\n",
       "      <td>0.063488</td>\n",
       "      <td>0.102400</td>\n",
       "      <td>0.228352</td>\n",
       "      <td>0.065536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11264</th>\n",
       "      <td>0.076800</td>\n",
       "      <td>0.119808</td>\n",
       "      <td>0.284672</td>\n",
       "      <td>0.091136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13312</th>\n",
       "      <td>0.090112</td>\n",
       "      <td>0.139264</td>\n",
       "      <td>0.320512</td>\n",
       "      <td>0.095232</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "name        qlora_addition_part  qlora_dense_part  qlora_forward  \\\n",
       "batch_size                                                         \n",
       "1024                   0.010240          0.027648       0.052224   \n",
       "3072                   0.023552          0.036864       0.081920   \n",
       "5120                   0.036864          0.054272       0.126976   \n",
       "7168                   0.050176          0.070656       0.169984   \n",
       "9216                   0.063488          0.102400       0.228352   \n",
       "11264                  0.076800          0.119808       0.284672   \n",
       "13312                  0.090112          0.139264       0.320512   \n",
       "\n",
       "name        qlora_low_rank_part  \n",
       "batch_size                       \n",
       "1024                   0.018432  \n",
       "3072                   0.025600  \n",
       "5120                   0.038912  \n",
       "7168                   0.051200  \n",
       "9216                   0.065536  \n",
       "11264                  0.091136  \n",
       "13312                  0.095232  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "params_grid = {\n",
    "    \"batch_size\": [1024 * i for i in range(1, 15, 2)],\n",
    "    \"hidden_size\": [768],\n",
    "    \"rank\": [128]\n",
    "}\n",
    "\n",
    "bench_input_list = profile.params_grid_to_list(params_grid)\n",
    "\n",
    "batch_size_result = profile.benchmark(\n",
    "    {\n",
    "        \"qlora_forward\": qlora_forward,\n",
    "        \"qlora_low_rank_part\": qlora_low_rank_part,\n",
    "        \"qlora_dense_part\": qlora_dense_part,\n",
    "        \"qlora_addition_part\": qlora_addition_part\n",
    "    },\n",
    "    bench_input_list\n",
    ")\n",
    "\n",
    "pivoted = batch_size_result.pivot(index=\"batch_size\", columns=\"name\", values=\"ms\")\n",
    "\n",
    "pivoted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>name</th>\n",
       "      <th>qlora_addition_part</th>\n",
       "      <th>qlora_dense_part</th>\n",
       "      <th>qlora_forward</th>\n",
       "      <th>qlora_low_rank_part</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>batch_size</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>0.039936</td>\n",
       "      <td>0.300032</td>\n",
       "      <td>0.384000</td>\n",
       "      <td>0.051200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3072</th>\n",
       "      <td>0.109568</td>\n",
       "      <td>0.911360</td>\n",
       "      <td>1.103872</td>\n",
       "      <td>0.107520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5120</th>\n",
       "      <td>0.179200</td>\n",
       "      <td>1.481728</td>\n",
       "      <td>1.815040</td>\n",
       "      <td>0.155648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7168</th>\n",
       "      <td>0.249776</td>\n",
       "      <td>2.183168</td>\n",
       "      <td>2.526208</td>\n",
       "      <td>0.202752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9216</th>\n",
       "      <td>0.318528</td>\n",
       "      <td>2.756608</td>\n",
       "      <td>3.318816</td>\n",
       "      <td>0.246784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11264</th>\n",
       "      <td>0.389120</td>\n",
       "      <td>3.346432</td>\n",
       "      <td>4.086784</td>\n",
       "      <td>0.307712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13312</th>\n",
       "      <td>0.458752</td>\n",
       "      <td>3.452416</td>\n",
       "      <td>4.233216</td>\n",
       "      <td>0.336896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "name        qlora_addition_part  qlora_dense_part  qlora_forward  \\\n",
       "batch_size                                                         \n",
       "1024                   0.039936          0.300032       0.384000   \n",
       "3072                   0.109568          0.911360       1.103872   \n",
       "5120                   0.179200          1.481728       1.815040   \n",
       "7168                   0.249776          2.183168       2.526208   \n",
       "9216                   0.318528          2.756608       3.318816   \n",
       "11264                  0.389120          3.346432       4.086784   \n",
       "13312                  0.458752          3.452416       4.233216   \n",
       "\n",
       "name        qlora_low_rank_part  \n",
       "batch_size                       \n",
       "1024                   0.051200  \n",
       "3072                   0.107520  \n",
       "5120                   0.155648  \n",
       "7168                   0.202752  \n",
       "9216                   0.246784  \n",
       "11264                  0.307712  \n",
       "13312                  0.336896  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "params_grid = {\n",
    "    \"batch_size\": [1024 * i for i in range(1, 15, 2)],\n",
    "    \"hidden_size\": [4096],\n",
    "    \"rank\": [128]\n",
    "}\n",
    "\n",
    "bench_input_list = profile.params_grid_to_list(params_grid)\n",
    "\n",
    "batch_size_result = profile.benchmark(\n",
    "    {\n",
    "        \"qlora_forward\": qlora_forward,\n",
    "        \"qlora_low_rank_part\": qlora_low_rank_part,\n",
    "        \"qlora_dense_part\": qlora_dense_part,\n",
    "        \"qlora_addition_part\": qlora_addition_part\n",
    "    },\n",
    "    bench_input_list\n",
    ")\n",
    "\n",
    "pivoted = batch_size_result.pivot(index=\"batch_size\", columns=\"name\", values=\"ms\")\n",
    "\n",
    "pivoted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>name</th>\n",
       "      <th>qlora_addition_part</th>\n",
       "      <th>qlora_dense_part</th>\n",
       "      <th>qlora_forward</th>\n",
       "      <th>qlora_low_rank_part</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hidden_size</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>0.039936</td>\n",
       "      <td>0.084992</td>\n",
       "      <td>0.167936</td>\n",
       "      <td>0.040960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3072</th>\n",
       "      <td>0.109568</td>\n",
       "      <td>0.687104</td>\n",
       "      <td>0.891904</td>\n",
       "      <td>0.103856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5120</th>\n",
       "      <td>0.179200</td>\n",
       "      <td>1.704960</td>\n",
       "      <td>2.014208</td>\n",
       "      <td>0.152576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7168</th>\n",
       "      <td>0.249728</td>\n",
       "      <td>3.686912</td>\n",
       "      <td>4.058112</td>\n",
       "      <td>0.224256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9216</th>\n",
       "      <td>0.319488</td>\n",
       "      <td>5.302272</td>\n",
       "      <td>5.871616</td>\n",
       "      <td>0.304128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "name         qlora_addition_part  qlora_dense_part  qlora_forward  \\\n",
       "hidden_size                                                         \n",
       "1024                    0.039936          0.084992       0.167936   \n",
       "3072                    0.109568          0.687104       0.891904   \n",
       "5120                    0.179200          1.704960       2.014208   \n",
       "7168                    0.249728          3.686912       4.058112   \n",
       "9216                    0.319488          5.302272       5.871616   \n",
       "\n",
       "name         qlora_low_rank_part  \n",
       "hidden_size                       \n",
       "1024                    0.040960  \n",
       "3072                    0.103856  \n",
       "5120                    0.152576  \n",
       "7168                    0.224256  \n",
       "9216                    0.304128  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "params_grid = {\n",
    "    \"batch_size\": [4096],\n",
    "    \"hidden_size\": [1024 * i for i in range(1, 10, 2)],\n",
    "    \"rank\": [128]\n",
    "}\n",
    "\n",
    "bench_input_list = profile.params_grid_to_list(params_grid)\n",
    "\n",
    "batch_size_result = profile.benchmark(\n",
    "    {\n",
    "        \"qlora_forward\": qlora_forward,\n",
    "        \"qlora_low_rank_part\": qlora_low_rank_part,\n",
    "        \"qlora_dense_part\": qlora_dense_part,\n",
    "        \"qlora_addition_part\": qlora_addition_part\n",
    "    },\n",
    "    bench_input_list\n",
    ")\n",
    "\n",
    "pivoted = batch_size_result.pivot(index=\"hidden_size\", columns=\"name\", values=\"ms\")\n",
    "\n",
    "pivoted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from merged_forward_v1 import merged_qlora_forward as v1\n",
    "from merged_forward_v2 import merged_qlora_forward as v2\n",
    "from merged_forward_v3 import merged_qlora_forward as v3\n",
    "\n",
    "from triton_dense_v1 import triton_dense_forward as dense_v1\n",
    "\n",
    "\n",
    "\n",
    "class merged_qlora_forward_v1(profile.BenchmarkCandidate):\n",
    "    def __init__(self, batch_size, hidden_size, rank):\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rank = rank\n",
    "        self.X = torch.randn((batch_size, hidden_size), device='cuda', dtype=torch.bfloat16)\n",
    "        self.W = torch.randn((hidden_size, hidden_size), device='cuda', dtype=torch.bfloat16)\n",
    "        self.U = torch.randn((hidden_size, rank), device='cuda', dtype=torch.bfloat16)\n",
    "        self.V = torch.randn((rank, hidden_size), device='cuda', dtype=torch.bfloat16)\n",
    "\n",
    "    def benchmark_content(self):\n",
    "        return v1(self.X, self.W, self.U, self.V)\n",
    "\n",
    "class merged_qlora_forward_v2(profile.BenchmarkCandidate):\n",
    "    def __init__(self, batch_size, hidden_size, rank):\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rank = rank\n",
    "        self.X = torch.randn((batch_size, hidden_size), device='cuda', dtype=torch.bfloat16)\n",
    "        self.W = torch.randn((hidden_size, hidden_size), device='cuda', dtype=torch.bfloat16)\n",
    "        self.U = torch.randn((hidden_size, rank), device='cuda', dtype=torch.bfloat16)\n",
    "        self.V = torch.randn((rank, hidden_size), device='cuda', dtype=torch.bfloat16)\n",
    "\n",
    "    def benchmark_content(self):\n",
    "        return v2(self.X, self.W, self.U, self.V)\n",
    "\n",
    "class merged_qlora_forward_v3(profile.BenchmarkCandidate):\n",
    "    def __init__(self, batch_size, hidden_size, rank):\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rank = rank\n",
    "        self.X = torch.randn((batch_size, hidden_size), device='cuda', dtype=torch.float16)\n",
    "        self.W = torch.randn((hidden_size, hidden_size), device='cuda', dtype=torch.float16)\n",
    "        self.U = torch.randn((hidden_size, rank), device='cuda', dtype=torch.float16)\n",
    "        self.V = torch.randn((rank, hidden_size), device='cuda', dtype=torch.float16)\n",
    "\n",
    "    def benchmark_content(self):\n",
    "        return v3(self.X, self.W, self.U, self.V)\n",
    "\n",
    "class triton_dense_v1(profile.BenchmarkCandidate):\n",
    "    def __init__(self, batch_size, hidden_size, rank):\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rank = rank\n",
    "        self.X = torch.randn((batch_size, hidden_size), device='cuda', dtype=torch.float16)\n",
    "        self.W = torch.randn((hidden_size, hidden_size), device='cuda', dtype=torch.float16)\n",
    "        self.U = torch.randn((hidden_size, rank), device='cuda', dtype=torch.float16)\n",
    "        self.V = torch.randn((rank, hidden_size), device='cuda', dtype=torch.float16)\n",
    "\n",
    "    def benchmark_content(self):\n",
    "        return dense_v1(self.X, self.W)\n",
    "    \n",
    "\n",
    "candidates = {\n",
    "    \"qlora_forward\": qlora_forward,\n",
    "    \"qlora_low_rank_part\": qlora_low_rank_part,\n",
    "    \"qlora_dense_part\": qlora_dense_part,\n",
    "    \"qlora_addition_part\": qlora_addition_part,\n",
    "    \"merged_qlora_forward_v1\": merged_qlora_forward_v1,\n",
    "    \"merged_qlora_forward_v2\": merged_qlora_forward_v2,\n",
    "    \"merged_qlora_forward_v3\": merged_qlora_forward_v3,\n",
    "    \"triton_dense_v1\": triton_dense_v1,\n",
    "}\n",
    "\n",
    "only_my_impl = {\n",
    "    \"baseline\": qlora_forward,\n",
    "    \"merged_qlora_forward_v1\": merged_qlora_forward_v1,\n",
    "    \"merged_qlora_forward_v2\": merged_qlora_forward_v2,\n",
    "    \"merged_qlora_forward_v3\": merged_qlora_forward_v3,\n",
    "    \"triton_dense_v1\": triton_dense_v1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autotuning results for function merged_qlora_forward_kernel (1024, 768, 768, 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.030719999223947525, 0.030719999223947525, 0.030719999223947525]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.030719999223947525, 0.02969600073993206, 0.030719999223947525]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.03276799991726875, 0.03174399957060814, 0.03276799991726875]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.03276799991726875, 0.03276799991726875, 0.03379200026392937]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.03379200026392937, 0.03276799991726875, 0.03379200026392937]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.035840000957250595, 0.035840000957250595, 0.035916801542043686]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.03993599861860275, 0.03993599861860275, 0.03993599861860275]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.03276799991726875, 0.03271679952740669, 0.03276799991726875]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.03174399957060814, 0.03174399957060814, 0.03276799991726875]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.035840000957250595, 0.035840000957250595, 0.03686400130391121]\n",
      "Autotuning results for function merged_qlora_forward_kernel (1024, 768, 768, 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.03174399957060814, 0.03174399957060814, 0.03174399957060814]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.030719999223947525, 0.030719999223947525, 0.03174399957060814]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.03276799991726875, 0.03174399957060814, 0.03276799991726875]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.04095999896526337, 0.04095999896526337, 0.04198399931192398]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.03481600061058998, 0.03481600061058998, 0.03481600061058998]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.04403200000524521, 0.04396799951791763, 0.04403200000524521]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.055296000093221664, 0.055296000093221664, 0.05632000043988228]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.03174399957060814, 0.03174399957060814, 0.03253759816288948]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.035840000957250595, 0.035840000957250595, 0.035840000957250595]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.03686400130391121, 0.035840000957250595, 0.03686400130391121]\n",
      "Autotuning results for function merged_qlora_forward_kernel (1024, 768, 768, 'torch.float16', 'torch.float16', 'torch.float16', 'torch.float16', 'torch.float16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.04095999896526337, 0.04095999896526337, 0.04198399931192398]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.04915200173854828, 0.04915200173854828, 0.050175998359918594]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.04505600035190582, 0.04505600035190582, 0.04608000069856644]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.04608000069856644, 0.04505600035190582, 0.04608000069856644]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.03481600061058998, 0.03379200026392937, 0.03481600061058998]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.03788800165057182, 0.03686400130391121, 0.03788800165057182]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.05222399905323982, 0.05119999870657921, 0.05222399905323982]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.04095999896526337, 0.04095999896526337, 0.04095999896526337]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.04608000069856644, 0.04505600035190582, 0.04608000069856644]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.035840000957250595, 0.035840000957250595, 0.03686400130391121]\n",
      "Autotuning results for function triton_dense_forward_kernel (1024, 768, 768, 'torch.float16', 'torch.float16', 'torch.float16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [0.03481600061058998, 0.03478400036692619, 0.03481600061058998]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.01945599913597107, 0.01945599913597107, 0.01945599913597107]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.01945599913597107, 0.01945599913597107, 0.020479999482631683]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.01945599913597107, 0.01945599913597107, 0.01945599913597107]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.020479999482631683, 0.020479999482631683, 0.020479999482631683]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.02457600086927414, 0.023552000522613525, 0.02457600086927414]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.02969600073993206, 0.028672000393271446, 0.02969600073993206]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.03174399957060814, 0.03174399957060814, 0.03276799991726875]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.026623999699950218, 0.025599999353289604, 0.026623999699950218]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.025599999353289604, 0.02457600086927414, 0.025599999353289604]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.027648000046610832, 0.027648000046610832, 0.027648000046610832]\n",
      "Autotuning results for function merged_qlora_forward_kernel (3072, 768, 768, 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.05939200147986412, 0.058368001133203506, 0.05939200147986412]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.058368001133203506, 0.05734400078654289, 0.058368001133203506]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.060416001826524734, 0.060416001826524734, 0.060416001826524734]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.06252799928188324, 0.062463998794555664, 0.06348799914121628]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.07372800260782242, 0.0727040022611618, 0.07372800260782242]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.09625600278377533, 0.096185602247715, 0.097120001912117]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.10342399775981903, 0.10239999741315842, 0.10444799810647964]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.06348799914121628, 0.06348799914121628, 0.06451199948787689]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.06656000018119812, 0.0655359998345375, 0.06656000018119812]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.08396799862384796, 0.08300799876451492, 0.08499199897050858]\n",
      "Autotuning results for function merged_qlora_forward_kernel (3072, 768, 768, 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.07168000191450119, 0.07065600156784058, 0.0727040022611618]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.05939200147986412, 0.05939200147986412, 0.060416001826524734]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.062463998794555664, 0.062463998794555664, 0.062463998794555664]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.08499199897050858, 0.08396799862384796, 0.08499199897050858]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.07372800260782242, 0.07372800260782242, 0.07475200295448303]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.12083200365304947, 0.12083200365304947, 0.12185599654912949]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.15462400019168854, 0.15360000729560852, 0.15462400019168854]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.062463998794555664, 0.062463998794555664, 0.06348799914121628]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.0727040022611618, 0.07168000191450119, 0.0727040022611618]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.0870399996638298, 0.08601599931716919, 0.08806400001049042]\n",
      "Autotuning results for function merged_qlora_forward_kernel (3072, 768, 768, 'torch.float16', 'torch.float16', 'torch.float16', 'torch.float16', 'torch.float16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.0798719972372055, 0.0798719972372055, 0.08089599758386612]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.09625600278377533, 0.09625600278377533, 0.09728000313043594]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.08601599931716919, 0.08499199897050858, 0.08601599931716919]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.08908800035715103, 0.08806400001049042, 0.08908800035715103]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.07577600330114365, 0.07574400305747986, 0.07613441348075867]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.10035199671983719, 0.10025600343942642, 0.1013759970664978]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.11878400295972824, 0.11878400295972824, 0.11980800330638885]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.08089599758386612, 0.08089599758386612, 0.08191999793052673]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.08908800035715103, 0.08806400001049042, 0.08908800035715103]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.08403199911117554, 0.08396799862384796, 0.08499199897050858]\n",
      "Autotuning results for function triton_dense_forward_kernel (3072, 768, 768, 'torch.float16', 'torch.float16', 'torch.float16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [0.03686400130391121, 0.03686400130391121, 0.03788800165057182]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.03993599861860275, 0.03993599861860275, 0.04095999896526337]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.035840000957250595, 0.03481600061058998, 0.035840000957250595]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.04198399931192398, 0.04198399931192398, 0.043007999658584595]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.04608000069856644, 0.04505600035190582, 0.04608000069856644]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.060416001826524734, 0.05939200147986412, 0.060416001826524734]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.07065600156784058, 0.07065600156784058, 0.07168000191450119]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.07782399654388428, 0.07782399654388428, 0.07884799689054489]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.04915200173854828, 0.048128001391887665, 0.04915200173854828]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.050175998359918594, 0.04915200173854828, 0.050175998359918594]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.0655359998345375, 0.06451199948787689, 0.0655359998345375]\n",
      "Autotuning results for function merged_qlora_forward_kernel (5120, 768, 768, 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.08806400001049042, 0.08806400001049042, 0.08908800035715103]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.08601599931716919, 0.08601599931716919, 0.0870399996638298]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.08908800035715103, 0.08908800035715103, 0.08908800035715103]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.09216000139713287, 0.09216000139713287, 0.09216000139713287]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.11776000261306763, 0.11776000261306763, 0.11878400295972824]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.15052799880504608, 0.15052799880504608, 0.1515520066022873]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.1525759994983673, 0.1525759994983673, 0.15360000729560852]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.09625600278377533, 0.0961088016629219, 0.09728000313043594]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.09523200243711472, 0.09523200243711472, 0.09625600278377533]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.13312000036239624, 0.13312000036239624, 0.13414399325847626]\n",
      "Autotuning results for function merged_qlora_forward_kernel (5120, 768, 768, 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.11468800157308578, 0.11468800157308578, 0.1157120019197464]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.08908800035715103, 0.08806400001049042, 0.08908800035715103]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.0942080020904541, 0.0942080020904541, 0.09523200243711472]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.12902399897575378, 0.12902399897575378, 0.130048006772995]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.12083200365304947, 0.12083200365304947, 0.12185599654912949]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.2027519941329956, 0.2017280012369156, 0.2027519941329956]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.26419198513031006, 0.26316800713539124, 0.26419198513031006]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.09318400174379349, 0.09318400174379349, 0.0942080020904541]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.11059200018644333, 0.10956799983978271, 0.11059200018644333]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.13926400244235992, 0.1382399946451187, 0.14028799533843994]\n",
      "Autotuning results for function merged_qlora_forward_kernel (5120, 768, 768, 'torch.float16', 'torch.float16', 'torch.float16', 'torch.float16', 'torch.float16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.11980800330638885, 0.11980800330638885, 0.12083200365304947]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.1443839967250824, 0.1443839967250824, 0.1443839967250824]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.12595200538635254, 0.12595200538635254, 0.12595200538635254]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.13312000036239624, 0.13312000036239624, 0.13411839306354523]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.12268800288438797, 0.12185599654912949, 0.1228799968957901]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.15462400019168854, 0.15360000729560852, 0.15564799308776855]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.1945280134677887, 0.19353599846363068, 0.1945600062608719]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.12291199713945389, 0.1228799968957901, 0.12390399724245071]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.13516800105571747, 0.13414399325847626, 0.13516800105571747]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.13619199395179749, 0.13516800105571747, 0.1372160017490387]\n",
      "Autotuning results for function triton_dense_forward_kernel (5120, 768, 768, 'torch.float16', 'torch.float16', 'torch.float16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [0.07065600156784058, 0.07065600156784058, 0.07168000191450119]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.06348799914121628, 0.062463998794555664, 0.06348799914121628]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.05222399905323982, 0.05222399905323982, 0.053247999399900436]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.06451199948787689, 0.06348799914121628, 0.06451199948787689]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.07016000151634216, 0.06963200122117996, 0.07065600156784058]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.09728000313043594, 0.09625600278377533, 0.09728000313043594]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.11878400295972824, 0.11776000261306763, 0.11878400295972824]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.12902399897575378, 0.12800000607967377, 0.12902399897575378]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.0727040022611618, 0.0727040022611618, 0.07289600372314453]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.07680000364780426, 0.07577600330114365, 0.07680000364780426]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.10449600219726562, 0.10444799810647964, 0.10547199845314026]\n",
      "Autotuning results for function merged_qlora_forward_kernel (7168, 768, 768, 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.11776000261306763, 0.11673600226640701, 0.11776000261306763]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.11468800157308578, 0.11366400122642517, 0.11468800157308578]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.11980800330638885, 0.11974400281906128, 0.12083200365304947]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.12492799758911133, 0.12390399724245071, 0.12595200538635254]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.16281600296497345, 0.16179199516773224, 0.16486400365829468]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.21196800470352173, 0.21094399690628052, 0.21606400609016418]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.2181120067834854, 0.2170879989862442, 0.22118400037288666]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.131071999669075, 0.130048006772995, 0.13209599256515503]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.131071999669075, 0.130048006772995, 0.13209599256515503]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.187391996383667, 0.18636800348758698, 0.18943999707698822]\n",
      "Autotuning results for function merged_qlora_forward_kernel (7168, 768, 768, 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.15564799308776855, 0.15462400019168854, 0.15564799308776855]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.11776000261306763, 0.11776000261306763, 0.11878400295972824]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.12902399897575378, 0.12800000607967377, 0.131071999669075]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.17510400712490082, 0.17510400712490082, 0.17612800002098083]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.16793599724769592, 0.1669120043516159, 0.17203199863433838]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.2805759906768799, 0.27852800488471985, 0.2819328010082245]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.3604480028152466, 0.3604480028152466, 0.3614720106124878]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.130048006772995, 0.12902399897575378, 0.131071999669075]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.14950400590896606, 0.14950400590896606, 0.1515520066022873]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.18943999707698822, 0.1884160041809082, 0.19148799777030945]\n",
      "Autotuning results for function merged_qlora_forward_kernel (7168, 768, 768, 'torch.float16', 'torch.float16', 'torch.float16', 'torch.float16', 'torch.float16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.1582079976797104, 0.15769599378108978, 0.158720001578331]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.19148799777030945, 0.19148799777030945, 0.19251200556755066]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.1669120043516159, 0.1658879965543747, 0.1669120043516159]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.17612800002098083, 0.17612800002098083, 0.17715199291706085]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.1669120043516159, 0.1658879965543747, 0.16895359754562378]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.21401600539684296, 0.21299199759960175, 0.22220799326896667]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.2662400007247925, 0.26521599292755127, 0.2662400007247925]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.16383999586105347, 0.16281600296497345, 0.16383999586105347]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.1802240014076233, 0.17919999361038208, 0.1802240014076233]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.18943999707698822, 0.1884160041809082, 0.19046400487422943]\n",
      "Autotuning results for function triton_dense_forward_kernel (7168, 768, 768, 'torch.float16', 'torch.float16', 'torch.float16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [0.07372800260782242, 0.0727040022611618, 0.07372800260782242]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.08499199897050858, 0.08499199897050858, 0.08601599931716919]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.06956800073385239, 0.06860800087451935, 0.06963200122117996]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.08606399595737457, 0.08601599931716919, 0.0870399996638298]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.09318400174379349, 0.09318400174379349, 0.0942080020904541]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.13209599256515503, 0.13209599256515503, 0.13414399325847626]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.16281600296497345, 0.16179199516773224, 0.16281600296497345]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.17761600017547607, 0.17715199291706085, 0.17817600071430206]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.09625600278377533, 0.09625600278377533, 0.09728000313043594]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.10239999741315842, 0.10239999741315842, 0.10342399775981903]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.14131200313568115, 0.14028799533843994, 0.14233599603176117]\n",
      "Autotuning results for function merged_qlora_forward_kernel (9216, 768, 768, 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.1669120043516159, 0.1658879965543747, 0.16896000504493713]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.16281600296497345, 0.16281600296497345, 0.16383999586105347]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.158720001578331, 0.1566976010799408, 0.15975040197372437]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.16281600296497345, 0.16179199516773224, 0.16383999586105347]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.20684799551963806, 0.20582400262355804, 0.21094399690628052]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.27033600211143494, 0.2682879865169525, 0.2779136300086975]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.2734079957008362, 0.2693119943141937, 0.2744320034980774]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.1669120043516159, 0.1669120043516159, 0.16793599724769592]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.1669120043516159, 0.1658879965543747, 0.16793599724769592]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.23244799673557281, 0.2303999960422516, 0.23552000522613525]\n",
      "Autotuning results for function merged_qlora_forward_kernel (9216, 768, 768, 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.2181120067834854, 0.2170879989862442, 0.21913599967956543]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.16896000504493713, 0.16793599724769592, 0.16998399794101715]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.17203199863433838, 0.17100800573825836, 0.1740799993276596]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.2314240038394928, 0.2314240038394928, 0.23347200453281403]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.22732800245285034, 0.21606400609016418, 0.22835199534893036]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.3635199964046478, 0.3635199964046478, 0.3696640133857727]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.47411200404167175, 0.47308799624443054, 0.47411200404167175]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.16998399794101715, 0.16896000504493713, 0.17203199863433838]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.20684799551963806, 0.20582400262355804, 0.20787200331687927]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.24063999950885773, 0.2385919988155365, 0.2448384016752243]\n",
      "Autotuning results for function merged_qlora_forward_kernel (9216, 768, 768, 'torch.float16', 'torch.float16', 'torch.float16', 'torch.float16', 'torch.float16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.22937600314617157, 0.2293248027563095, 0.22937600314617157]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.27852800488471985, 0.27852800488471985, 0.27955201268196106]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.2242559939622879, 0.2242559939622879, 0.2271232008934021]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.23747199773788452, 0.23654399812221527, 0.23756800591945648]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.21401600539684296, 0.21196800470352173, 0.2170879989862442]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.27238398790359497, 0.27033600211143494, 0.2769984006881714]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.3420160114765167, 0.3389439880847931, 0.344268798828125]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.21401600539684296, 0.21401600539684296, 0.21503999829292297]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.23654399812221527, 0.23552000522613525, 0.23654399812221527]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.23552000522613525, 0.23449599742889404, 0.23756800591945648]\n",
      "Autotuning results for function triton_dense_forward_kernel (9216, 768, 768, 'torch.float16', 'torch.float16', 'torch.float16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [0.10547199845314026, 0.10444799810647964, 0.10547199845314026]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.1157120019197464, 0.1157120019197464, 0.11673600226640701]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.09523200243711472, 0.0942080020904541, 0.09523200243711472]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.11264000087976456, 0.11260800063610077, 0.11468800157308578]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.1228799968957901, 0.1228799968957901, 0.12595200538635254]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.17100800573825836, 0.16998399794101715, 0.17307519912719727]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.2099200040102005, 0.2099200040102005, 0.2120639979839325]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.2303999960422516, 0.22937600314617157, 0.23183360695838928]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.12697599828243256, 0.12595200538635254, 0.12697599828243256]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.13300800323486328, 0.13209599256515503, 0.13312000036239624]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.1812479943037033, 0.1802240014076233, 0.18329599499702454]\n",
      "Autotuning results for function merged_qlora_forward_kernel (11264, 768, 768, 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.19763199985027313, 0.19660800695419312, 0.19865599274635315]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.19251200556755066, 0.19046400487422943, 0.19537919759750366]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.18943999707698822, 0.1884160041809082, 0.19287040829658508]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.1955839991569519, 0.1945600062608719, 0.19865599274635315]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.2549760043621063, 0.2519040107727051, 0.26009601354599]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.3256320059299469, 0.32256001234054565, 0.3321855962276459]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.3256320059299469, 0.3246079981327057, 0.3318016231060028]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.19971200823783875, 0.19865599274635315, 0.20582400262355804]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.19968000054359436, 0.19865599274635315, 0.2017280012369156]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.2815999984741211, 0.27955201268196106, 0.28672000765800476]\n",
      "Autotuning results for function merged_qlora_forward_kernel (11264, 768, 768, 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.2611199915409088, 0.2590720057487488, 0.26439037919044495]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.1995519995689392, 0.19763199985027313, 0.20167680084705353]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.20377600193023682, 0.2017280012369156, 0.2088959962129593]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.27136000990867615, 0.2693119943141937, 0.2805759906768799]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.2662400007247925, 0.26419198513031006, 0.27033600211143494]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.43724799156188965, 0.4362240135669708, 0.44441598653793335]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.5601279735565186, 0.5601279735565186, 0.5611519813537598]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.19865599274635315, 0.19660800695419312, 0.20070399343967438]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.24118399620056152, 0.2394687980413437, 0.24473600089550018]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.29286399483680725, 0.289792001247406, 0.2994367778301239]\n",
      "Autotuning results for function merged_qlora_forward_kernel (11264, 768, 768, 'torch.float16', 'torch.float16', 'torch.float16', 'torch.float16', 'torch.float16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.2672640085220337, 0.2672640085220337, 0.2682879865169525]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.3256320059299469, 0.3246079981327057, 0.3256320059299469]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.26521599292755127, 0.26419198513031006, 0.268697589635849]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.2805759906768799, 0.2805759906768799, 0.2815999984741211]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.26214399933815, 0.26009601354599, 0.26664960384368896]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.32767999172210693, 0.32355839014053345, 0.3328000009059906]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.41574400663375854, 0.4126720130443573, 0.4199039936065674]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.2549760043621063, 0.2539519965648651, 0.2549760043621063]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.2815999984741211, 0.2815999984741211, 0.2826240062713623]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.28467199206352234, 0.2826240062713623, 0.29102081060409546]\n",
      "Autotuning results for function triton_dense_forward_kernel (11264, 768, 768, 'torch.float16', 'torch.float16', 'torch.float16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [0.13414399325847626, 0.13414399325847626, 0.13516800105571747]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.13619199395179749, 0.13516800105571747, 0.13926400244235992]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.11059200018644333, 0.10956799983978271, 0.11257600039243698]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.13619199395179749, 0.13516800105571747, 0.14028799533843994]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.15052799880504608, 0.14847999811172485, 0.1515520066022873]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.21196800470352173, 0.2099200040102005, 0.21503999829292297]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.25804799795150757, 0.25702399015426636, 0.26316800713539124]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.27955201268196106, 0.27859199047088623, 0.2836480140686035]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.15462400019168854, 0.15360000729560852, 0.15667200088500977]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.16076800227165222, 0.159743994474411, 0.16281600296497345]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.22015999257564545, 0.2181120067834854, 0.2242559939622879]\n",
      "Autotuning results for function merged_qlora_forward_kernel (13312, 768, 768, 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.22734400629997253, 0.22732800245285034, 0.2303999960422516]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.22220799326896667, 0.22015999257564545, 0.22487039864063263]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.22015999257564545, 0.2181120067834854, 0.2252800017595291]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.22937600314617157, 0.22732800245285034, 0.23449599742889404]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.3020800054073334, 0.30105599761009216, 0.3092480003833771]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.3962880074977875, 0.3911679983139038, 0.40529921650886536]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.3973119854927063, 0.39423999190330505, 0.40386563539505005]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.23654399812221527, 0.23552000522613525, 0.23756800591945648]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.2396160066127777, 0.23756800591945648, 0.2453504055738449]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.33792001008987427, 0.33484798669815063, 0.3432447910308838]\n",
      "Autotuning results for function merged_qlora_forward_kernel (13312, 768, 768, 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.30720001459121704, 0.3051519989967346, 0.3112959861755371]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.22835199534893036, 0.22732800245285034, 0.23552000522613525]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.2457599937915802, 0.24166400730609894, 0.25088000297546387]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.32972800731658936, 0.3250176012516022, 0.3338240087032318]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.3184640109539032, 0.31436800956726074, 0.32153600454330444]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.5304319858551025, 0.5263360142707825, 0.5363711714744568]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.6799359917640686, 0.6799359917640686, 0.6809599995613098]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.2396160066127777, 0.23654399812221527, 0.24371199309825897]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.289792001247406, 0.2877439856529236, 0.29286399483680725]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.3389439880847931, 0.33689600229263306, 0.3456256091594696]\n",
      "Autotuning results for function merged_qlora_forward_kernel (13312, 768, 768, 'torch.float16', 'torch.float16', 'torch.float16', 'torch.float16', 'torch.float16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.3092480003833771, 0.30720001459121704, 0.3135488033294678]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.37376001477241516, 0.37273600697517395, 0.37401601672172546]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.30720001459121704, 0.3051519989967346, 0.3092480003833771]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.3266560137271881, 0.3256320059299469, 0.32972800731658936]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.30822399258613586, 0.30720001459121704, 0.31436800956726074]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.39526399970054626, 0.39321601390838623, 0.4034560024738312]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.49561598896980286, 0.49344637989997864, 0.502784013748169]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.30003198981285095, 0.29900801181793213, 0.30003198981285095]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.32972800731658936, 0.32870399951934814, 0.3338240087032318]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.3409920036792755, 0.33792001008987427, 0.34590721130371094]\n",
      "Autotuning results for function triton_dense_forward_kernel (13312, 768, 768, 'torch.float16', 'torch.float16', 'torch.float16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [0.14127999544143677, 0.1402495950460434, 0.14233599603176117]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.16076800227165222, 0.15769599378108978, 0.16281600296497345]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.12697599828243256, 0.12595200538635254, 0.12800000607967377]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.16076800227165222, 0.15769599378108978, 0.16486400365829468]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.17510400712490082, 0.1740799993276596, 0.17960961163043976]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.24883200228214264, 0.24683520197868347, 0.2539519965648651]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.3051519989967346, 0.30392318964004517, 0.31175681948661804]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.3307519853115082, 0.32767999172210693, 0.3317759931087494]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.17603200674057007, 0.17510400712490082, 0.17715199291706085]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.18636800348758698, 0.18534399569034576, 0.18943999707698822]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.25702399015426636, 0.2549760043621063, 0.26316800713539124]\n",
      "Autotuning results for function merged_qlora_forward_kernel (15360, 768, 768, 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.26316800713539124, 0.2611199915409088, 0.2662400007247925]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.2519040107727051, 0.24985599517822266, 0.2611199915409088]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.26009601354599, 0.25804799795150757, 0.26606079936027527]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.27033600211143494, 0.26705920696258545, 0.2747648060321808]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.36454400420188904, 0.347135990858078, 0.3665919899940491]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.4556800127029419, 0.45158401131629944, 0.4638720154762268]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.4607999920845032, 0.4567039906978607, 0.46592000126838684]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.27344000339508057, 0.27238398790359497, 0.2764799892902374]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.2744320034980774, 0.2734079957008362, 0.27852800488471985]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.3973119854927063, 0.3840000033378601, 0.40243199467658997]\n",
      "Autotuning results for function merged_qlora_forward_kernel (15360, 768, 768, 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.3481599986553192, 0.3420160114765167, 0.35737600922584534]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.25702399015426636, 0.2549760043621063, 0.26009601354599]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.2847999930381775, 0.2815999984741211, 0.29183998703956604]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.3829759955406189, 0.37887999415397644, 0.38927361369132996]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.36454400420188904, 0.35942399501800537, 0.37376001477241516]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.6072319746017456, 0.6055936217308044, 0.6215680241584778]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.7772160172462463, 0.7761920094490051, 0.7772160172462463]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.27750399708747864, 0.2734079957008362, 0.2805759906768799]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.33792001008987427, 0.33689600229263306, 0.34303998947143555]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.3891200125217438, 0.3850240111351013, 0.3962880074977875]\n",
      "Autotuning results for function merged_qlora_forward_kernel (15360, 768, 768, 'torch.float16', 'torch.float16', 'torch.float16', 'torch.float16', 'torch.float16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.3476479947566986, 0.34406399726867676, 0.35164159536361694]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.41676801443099976, 0.41574400663375854, 0.42188799381256104]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.3604480028152466, 0.35942399501800537, 0.3635199964046478]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.3819519877433777, 0.37887999415397644, 0.3850240111351013]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.3532800078392029, 0.35123199224472046, 0.3563520014286041]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.45158401131629944, 0.4485119879245758, 0.4607999920845032]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.5652480125427246, 0.5611519813537598, 0.5744640231132507]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.3409920036792755, 0.3407039940357208, 0.34508800506591797]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.3758080005645752, 0.374783992767334, 0.37785598635673523]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.3901439905166626, 0.38809600472450256, 0.39423999190330505]\n",
      "Autotuning results for function triton_dense_forward_kernel (15360, 768, 768, 'torch.float16', 'torch.float16', 'torch.float16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [0.16793599724769592, 0.1669120043516159, 0.17203199863433838]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.1791519969701767, 0.17715199291706085, 0.18227200210094452]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.14233599603176117, 0.14233599603176117, 0.14643199741840363]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.18227200210094452, 0.1812479943037033, 0.18903040885925293]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.19865599274635315, 0.19763199985027313, 0.2017280012369156]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.28468799591064453, 0.2815999984741211, 0.29286402463912964]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.35020801424980164, 0.3491584062576294, 0.35840001702308655]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.38092800974845886, 0.37483519315719604, 0.3829759955406189]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.2027519941329956, 0.20070399343967438, 0.20574720203876495]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.21196800470352173, 0.2099200040102005, 0.21503999829292297]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.29388800263404846, 0.2908160090446472, 0.2969599962234497]\n",
      "Autotuning results for function merged_qlora_forward_kernel (17408, 768, 768, 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.289792001247406, 0.2887679934501648, 0.29286399483680725]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.27955201268196106, 0.2764799892902374, 0.28569599986076355]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.2887679934501648, 0.2877439856529236, 0.2959359884262085]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.3031040132045746, 0.29900801181793213, 0.30822399258613586]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.3962880074977875, 0.3901439905166626, 0.406169593334198]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.5160959959030151, 0.506879985332489, 0.5296127796173096]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.5160959959030151, 0.5150719881057739, 0.5273600220680237]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.30617600679397583, 0.3031040132045746, 0.31191039085388184]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.3092480003833771, 0.3051519989967346, 0.31764480471611023]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.4331519901752472, 0.4302079975605011, 0.4413439929485321]\n",
      "Autotuning results for function merged_qlora_forward_kernel (17408, 768, 768, 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.3962880074977875, 0.3911679983139038, 0.40243199467658997]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.2877439856529236, 0.28569599986076355, 0.2906560003757477]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.32153600454330444, 0.319487988948822, 0.32870399951934814]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.43212801218032837, 0.42905598878860474, 0.4403199851512909]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.41676801443099976, 0.4106239974498749, 0.42618879675865173]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.6901760101318359, 0.6840320229530334, 0.7086079716682434]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.8796160221099854, 0.8775680065155029, 0.8796160221099854]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.3162879943847656, 0.3102720081806183, 0.32256001234054565]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.3840000033378601, 0.3819519877433777, 0.3871231973171234]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.447488009929657, 0.4433920085430145, 0.45260798931121826]\n",
      "Autotuning results for function merged_qlora_forward_kernel (17408, 768, 768, 'torch.float16', 'torch.float16', 'torch.float16', 'torch.float16', 'torch.float16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.3850240111351013, 0.3850240111351013, 0.3901439905166626]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.4628480076789856, 0.4628480076789856, 0.4679679870605469]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.4044800102710724, 0.40140798687934875, 0.4087808132171631]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.4270080029964447, 0.4259839951992035, 0.43253761529922485]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.40140798687934875, 0.3973119854927063, 0.40857601165771484]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.5150719881057739, 0.5139520168304443, 0.5253120064735413]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.638975977897644, 0.6379520297050476, 0.6533120274543762]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.3850240111351013, 0.3840000033378601, 0.3871679902076721]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.4270080029964447, 0.4259839951992035, 0.43007999658584595]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.43827199935913086, 0.4362240135669708, 0.4413439929485321]\n",
      "Autotuning results for function triton_dense_forward_kernel (17408, 768, 768, 'torch.float16', 'torch.float16', 'torch.float16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [0.17510400712490082, 0.1740799993276596, 0.17940479516983032]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.20479999482631683, 0.20069119334220886, 0.2091519981622696]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.16076800227165222, 0.159743994474411, 0.16281600296497345]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.20684799551963806, 0.20479999482631683, 0.21196800470352173]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.2252800017595291, 0.22323200106620789, 0.23449599742889404]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.3246079981327057, 0.319487988948822, 0.32767999172210693]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.3983359932899475, 0.3940351903438568, 0.40243199467658997]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.43007999658584595, 0.4270080029964447, 0.4362240135669708]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.2242559939622879, 0.22220799326896667, 0.22732800245285034]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.24063999950885773, 0.2385919988155365, 0.24739840626716614]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.3338240087032318, 0.3307519853115082, 0.34160640835762024]\n",
      "Autotuning results for function merged_qlora_forward_kernel (19456, 768, 768, 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.3256320059299469, 0.3235839903354645, 0.3303423821926117]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.31539198756217957, 0.31334400177001953, 0.32061439752578735]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.3246079981327057, 0.31969279050827026, 0.3307519853115082]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.34406399726867676, 0.3389439880847931, 0.35020801424980164]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.4433920085430145, 0.4366336166858673, 0.45260798931121826]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.5785920023918152, 0.5713919997215271, 0.5857920050621033]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.579584002494812, 0.5765119791030884, 0.5908480286598206]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.3409920036792755, 0.3399679958820343, 0.34303998947143555]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.35121601819992065, 0.3461120128631592, 0.35942399501800537]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.4853760004043579, 0.48127999901771545, 0.49663999676704407]\n",
      "Autotuning results for function merged_qlora_forward_kernel (19456, 768, 768, 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.44441598653793335, 0.4384768009185791, 0.45445120334625244]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.3266560137271881, 0.32153600454330444, 0.3324671983718872]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.3604480028152466, 0.3573184013366699, 0.368230402469635]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.4843519926071167, 0.48025599122047424, 0.4935680031776428]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.4700160026550293, 0.45772799849510193, 0.48025599122047424]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.7700480222702026, 0.7655423879623413, 0.7956479787826538]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.9820160269737244, 0.9818623661994934, 0.9891840219497681]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.34406399726867676, 0.3389439880847931, 0.3491840064525604]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.43110400438308716, 0.42800000309944153, 0.43540480732917786]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.4976640045642853, 0.4904960095882416, 0.5093376040458679]\n",
      "Autotuning results for function merged_qlora_forward_kernel (19456, 768, 768, 'torch.float16', 'torch.float16', 'torch.float16', 'torch.float16', 'torch.float16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.42905598878860474, 0.4249599874019623, 0.4341759979724884]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.5160959959030151, 0.5120000243186951, 0.5212159752845764]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.447488009929657, 0.4433920085430145, 0.45158401131629944]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.477183997631073, 0.47308799624443054, 0.48127999901771545]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.4485119879245758, 0.44298240542411804, 0.45772799849510193]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.5765119791030884, 0.5713919997215271, 0.5816320180892944]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.719871997833252, 0.7178239822387695, 0.7258111834526062]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.42905598878860474, 0.4280320107936859, 0.4356095790863037]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.47513601183891296, 0.47411200404167175, 0.4853760004043579]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.4904960095882416, 0.48742398619651794, 0.4976640045642853]\n",
      "Autotuning results for function triton_dense_forward_kernel (19456, 768, 768, 'torch.float16', 'torch.float16', 'torch.float16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [0.20479999482631683, 0.20377600193023682, 0.20684799551963806]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.22729599475860596, 0.22423680126667023, 0.2314240038394928]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.1812479943037033, 0.17919999361038208, 0.18636800348758698]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.23449599742889404, 0.2314240038394928, 0.23756800591945648]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.25600001215934753, 0.2519040107727051, 0.2611199915409088]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.3676159977912903, 0.36556801199913025, 0.374783992767334]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.447488009929657, 0.44646400213241577, 0.4567039906978607]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.48127999901771545, 0.48025599122047424, 0.48444798588752747]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.25088000297546387, 0.24985599517822266, 0.2529279887676239]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.27238398790359497, 0.2699072062969208, 0.2811904549598694]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.3758080005645752, 0.37171199917793274, 0.38318079710006714]\n",
      "Autotuning results for function merged_qlora_forward_kernel (21504, 768, 768, 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.35839998722076416, 0.35737600922584534, 0.35942399501800537]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.3491840064525604, 0.34508800506591797, 0.3571712076663971]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.3604480028152466, 0.3553279936313629, 0.36802560091018677]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.3814399838447571, 0.374783992767334, 0.3891200125217438]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.4904960095882416, 0.4833280146121979, 0.5017600059509277]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.6399999856948853, 0.6338559985160828, 0.6502400040626526]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.6410239934921265, 0.638975977897644, 0.652288019657135]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.3819519877433777, 0.37990400195121765, 0.38604798913002014]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.38707199692726135, 0.3840000033378601, 0.39321601390838623]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.5406720042228699, 0.536575973033905, 0.5457919836044312]\n",
      "Autotuning results for function merged_qlora_forward_kernel (21504, 768, 768, 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.4904960095882416, 0.4864000082015991, 0.49459201097488403]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.3563520014286041, 0.3532800078392029, 0.3624959886074066]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.4014720022678375, 0.3973119854927063, 0.40673279762268066]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.5345280170440674, 0.5335040092468262, 0.5459967851638794]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.5263360142707825, 0.5171200037002563, 0.54415363073349]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.8560640215873718, 0.8488960266113281, 0.8747007846832275]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [1.0833920240402222, 1.0833920240402222, 1.0915839672088623]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.39321601390838623, 0.3875904083251953, 0.4044800102710724]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.4782400131225586, 0.47513601183891296, 0.4853760004043579]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.5519359707832336, 0.5468159914016724, 0.5623807907104492]\n",
      "Autotuning results for function merged_qlora_forward_kernel (21504, 768, 768, 'torch.float16', 'torch.float16', 'torch.float16', 'torch.float16', 'torch.float16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.4700160026550293, 0.4689919948577881, 0.48025599122047424]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.5631999969482422, 0.562175989151001, 0.5685247778892517]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.4925439953804016, 0.48947200179100037, 0.4986880123615265]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.5242879986763, 0.5232639908790588, 0.5289983749389648]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.4997119903564453, 0.4935680031776428, 0.5107712149620056]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.638975977897644, 0.6359040141105652, 0.6492159962654114]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.7961599826812744, 0.7895039916038513, 0.8028159737586975]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.4761599898338318, 0.47308799624443054, 0.48230400681495667]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.5273600220680237, 0.5232639908790588, 0.532480001449585]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.5468159914016724, 0.5427200198173523, 0.5539839863777161]\n",
      "Autotuning results for function triton_dense_forward_kernel (21504, 768, 768, 'torch.float16', 'torch.float16', 'torch.float16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [0.21503999829292297, 0.21299199759960175, 0.2252800017595291]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.25088000297546387, 0.24780799448490143, 0.25308799743652344]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.2011680006980896, 0.19865599274635315, 0.20684799551963806]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.26009601354599, 0.25804799795150757, 0.2662400007247925]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.2836480140686035, 0.27955201268196106, 0.2908160090446472]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.4044800102710724, 0.40140798687934875, 0.41390082240104675]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.49663999676704407, 0.4915199875831604, 0.5021696090698242]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.5335040092468262, 0.531391978263855, 0.5355520248413086]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.27955201268196106, 0.2754560112953186, 0.28753918409347534]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.3020800054073334, 0.2979840040206909, 0.30904319882392883]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.4116480052471161, 0.4065279960632324, 0.4179967939853668]\n",
      "Autotuning results for function merged_qlora_forward_kernel (23552, 768, 768, 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.40755200386047363, 0.40140798687934875, 0.41471999883651733]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.39526399970054626, 0.3911679983139038, 0.40243199467658997]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.3962880074977875, 0.39321601390838623, 0.4044800102710724]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.42188799381256104, 0.41574400663375854, 0.43007999658584595]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.536575973033905, 0.532480001449585, 0.5537792444229126]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.6993920207023621, 0.692243218421936, 0.711679995059967]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.7055360078811646, 0.6993920207023621, 0.7127040028572083]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.4177919924259186, 0.4136959910392761, 0.42393600940704346]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.4280320107936859, 0.4249599874019623, 0.4352000057697296]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.5990399718284607, 0.5939199924468994, 0.6072319746017456]\n",
      "Autotuning results for function merged_qlora_forward_kernel (23552, 768, 768, 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.5519359707832336, 0.5468159914016724, 0.5652480125427246]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.4065279960632324, 0.4034560024738312, 0.41471999883651733]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.4433920085430145, 0.43929600715637207, 0.4559231996536255]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.5949440002441406, 0.5928959846496582, 0.605184018611908]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.5715199708938599, 0.562175989151001, 0.5842943787574768]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.9379839897155762, 0.9308159947395325, 0.9615359902381897]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [1.18886399269104, 1.18886399269104, 1.1898880004882812]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.43110400438308716, 0.42188799381256104, 0.4423680007457733]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.5360640287399292, 0.5314559936523438, 0.5437056422233582]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.6010879874229431, 0.5959680080413818, 0.6139135956764221]\n",
      "Autotuning results for function merged_qlora_forward_kernel (23552, 768, 768, 'torch.float16', 'torch.float16', 'torch.float16', 'torch.float16', 'torch.float16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.5396479964256287, 0.5345280170440674, 0.5486592054367065]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.647167980670929, 0.6461439728736877, 0.6536704301834106]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.5488640069961548, 0.5437440276145935, 0.5539647936820984]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.5847039818763733, 0.5836799740791321, 0.5939199924468994]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.5457919836044312, 0.5406720042228699, 0.5519359707832336]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.7045120000839233, 0.6983680129051208, 0.7077888250350952]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.8704000115394592, 0.8652799725532532, 0.8857600092887878]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.52019202709198, 0.5191680192947388, 0.5273600220680237]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.5806080102920532, 0.5765119791030884, 0.5879808068275452]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.6133760213851929, 0.5980160236358643, 0.6164479851722717]\n",
      "Autotuning results for function triton_dense_forward_kernel (23552, 768, 768, 'torch.float16', 'torch.float16', 'torch.float16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [0.24371199309825897, 0.24166400730609894, 0.2494272142648697]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.27852800488471985, 0.2719999849796295, 0.28610560297966003]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.22310400009155273, 0.21975040435791016, 0.22937600314617157]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.28672000765800476, 0.2822144031524658, 0.2902016043663025]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.3164159953594208, 0.3108479976654053, 0.3235839903354645]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.44543999433517456, 0.4413439929485321, 0.45547521114349365]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.5416960120201111, 0.5406720042228699, 0.5519359707832336]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.5857279896736145, 0.5806080102920532, 0.5944575667381287]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.3051519989967346, 0.30003198981285095, 0.3112959861755371]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.3338240087032318, 0.3305791914463043, 0.3409664034843445]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.45158401131629944, 0.447488009929657, 0.4607999920845032]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>name</th>\n",
       "      <th>merged_qlora_forward_v1</th>\n",
       "      <th>merged_qlora_forward_v2</th>\n",
       "      <th>merged_qlora_forward_v3</th>\n",
       "      <th>qlora_addition_part</th>\n",
       "      <th>qlora_dense_part</th>\n",
       "      <th>qlora_forward</th>\n",
       "      <th>qlora_low_rank_part</th>\n",
       "      <th>triton_dense_v1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>batch_size</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>0.030720</td>\n",
       "      <td>0.030800</td>\n",
       "      <td>0.034816</td>\n",
       "      <td>0.010240</td>\n",
       "      <td>0.028672</td>\n",
       "      <td>0.057344</td>\n",
       "      <td>0.026624</td>\n",
       "      <td>0.019456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3072</th>\n",
       "      <td>0.058368</td>\n",
       "      <td>0.060416</td>\n",
       "      <td>0.075776</td>\n",
       "      <td>0.024576</td>\n",
       "      <td>0.036864</td>\n",
       "      <td>0.077824</td>\n",
       "      <td>0.023552</td>\n",
       "      <td>0.034848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5120</th>\n",
       "      <td>0.086016</td>\n",
       "      <td>0.089088</td>\n",
       "      <td>0.119808</td>\n",
       "      <td>0.037888</td>\n",
       "      <td>0.054272</td>\n",
       "      <td>0.121856</td>\n",
       "      <td>0.033792</td>\n",
       "      <td>0.052224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7168</th>\n",
       "      <td>0.114688</td>\n",
       "      <td>0.117760</td>\n",
       "      <td>0.158720</td>\n",
       "      <td>0.050176</td>\n",
       "      <td>0.071680</td>\n",
       "      <td>0.160768</td>\n",
       "      <td>0.043008</td>\n",
       "      <td>0.069632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9216</th>\n",
       "      <td>0.157696</td>\n",
       "      <td>0.168960</td>\n",
       "      <td>0.215040</td>\n",
       "      <td>0.063488</td>\n",
       "      <td>0.103424</td>\n",
       "      <td>0.215040</td>\n",
       "      <td>0.052224</td>\n",
       "      <td>0.095232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11264</th>\n",
       "      <td>0.189440</td>\n",
       "      <td>0.197632</td>\n",
       "      <td>0.254976</td>\n",
       "      <td>0.076800</td>\n",
       "      <td>0.120832</td>\n",
       "      <td>0.254976</td>\n",
       "      <td>0.061440</td>\n",
       "      <td>0.110592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13312</th>\n",
       "      <td>0.220160</td>\n",
       "      <td>0.230400</td>\n",
       "      <td>0.300032</td>\n",
       "      <td>0.090112</td>\n",
       "      <td>0.139264</td>\n",
       "      <td>0.295936</td>\n",
       "      <td>0.070656</td>\n",
       "      <td>0.126976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15360</th>\n",
       "      <td>0.248832</td>\n",
       "      <td>0.258048</td>\n",
       "      <td>0.340992</td>\n",
       "      <td>0.102400</td>\n",
       "      <td>0.159744</td>\n",
       "      <td>0.335872</td>\n",
       "      <td>0.078848</td>\n",
       "      <td>0.143360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17408</th>\n",
       "      <td>0.278528</td>\n",
       "      <td>0.287744</td>\n",
       "      <td>0.386048</td>\n",
       "      <td>0.115712</td>\n",
       "      <td>0.181264</td>\n",
       "      <td>0.384000</td>\n",
       "      <td>0.087040</td>\n",
       "      <td>0.160768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19456</th>\n",
       "      <td>0.314368</td>\n",
       "      <td>0.323584</td>\n",
       "      <td>0.428032</td>\n",
       "      <td>0.129024</td>\n",
       "      <td>0.201728</td>\n",
       "      <td>0.423936</td>\n",
       "      <td>0.095232</td>\n",
       "      <td>0.181248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21504</th>\n",
       "      <td>0.346112</td>\n",
       "      <td>0.357376</td>\n",
       "      <td>0.472064</td>\n",
       "      <td>0.142336</td>\n",
       "      <td>0.223232</td>\n",
       "      <td>0.464896</td>\n",
       "      <td>0.104448</td>\n",
       "      <td>0.199680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23552</th>\n",
       "      <td>0.393120</td>\n",
       "      <td>0.410624</td>\n",
       "      <td>0.520192</td>\n",
       "      <td>0.155648</td>\n",
       "      <td>0.254976</td>\n",
       "      <td>0.517120</td>\n",
       "      <td>0.113664</td>\n",
       "      <td>0.222208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "name        merged_qlora_forward_v1  merged_qlora_forward_v2  \\\n",
       "batch_size                                                     \n",
       "1024                       0.030720                 0.030800   \n",
       "3072                       0.058368                 0.060416   \n",
       "5120                       0.086016                 0.089088   \n",
       "7168                       0.114688                 0.117760   \n",
       "9216                       0.157696                 0.168960   \n",
       "11264                      0.189440                 0.197632   \n",
       "13312                      0.220160                 0.230400   \n",
       "15360                      0.248832                 0.258048   \n",
       "17408                      0.278528                 0.287744   \n",
       "19456                      0.314368                 0.323584   \n",
       "21504                      0.346112                 0.357376   \n",
       "23552                      0.393120                 0.410624   \n",
       "\n",
       "name        merged_qlora_forward_v3  qlora_addition_part  qlora_dense_part  \\\n",
       "batch_size                                                                   \n",
       "1024                       0.034816             0.010240          0.028672   \n",
       "3072                       0.075776             0.024576          0.036864   \n",
       "5120                       0.119808             0.037888          0.054272   \n",
       "7168                       0.158720             0.050176          0.071680   \n",
       "9216                       0.215040             0.063488          0.103424   \n",
       "11264                      0.254976             0.076800          0.120832   \n",
       "13312                      0.300032             0.090112          0.139264   \n",
       "15360                      0.340992             0.102400          0.159744   \n",
       "17408                      0.386048             0.115712          0.181264   \n",
       "19456                      0.428032             0.129024          0.201728   \n",
       "21504                      0.472064             0.142336          0.223232   \n",
       "23552                      0.520192             0.155648          0.254976   \n",
       "\n",
       "name        qlora_forward  qlora_low_rank_part  triton_dense_v1  \n",
       "batch_size                                                       \n",
       "1024             0.057344             0.026624         0.019456  \n",
       "3072             0.077824             0.023552         0.034848  \n",
       "5120             0.121856             0.033792         0.052224  \n",
       "7168             0.160768             0.043008         0.069632  \n",
       "9216             0.215040             0.052224         0.095232  \n",
       "11264            0.254976             0.061440         0.110592  \n",
       "13312            0.295936             0.070656         0.126976  \n",
       "15360            0.335872             0.078848         0.143360  \n",
       "17408            0.384000             0.087040         0.160768  \n",
       "19456            0.423936             0.095232         0.181248  \n",
       "21504            0.464896             0.104448         0.199680  \n",
       "23552            0.517120             0.113664         0.222208  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "params_grid = {\n",
    "    \"batch_size\": [1024 * i for i in range(1, 25, 2)],\n",
    "    \"hidden_size\": [768],\n",
    "    \"rank\": [16]\n",
    "}\n",
    "\n",
    "bench_input_list = profile.params_grid_to_list(params_grid)\n",
    "batch_size_result = profile.benchmark( candidates, bench_input_list)\n",
    "\n",
    "pivoted = batch_size_result.pivot(index=\"batch_size\", columns=\"name\", values=\"ms\")\n",
    "pivoted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autotuning results for function merged_qlora_forward_kernel (1024, 2048, 2048, 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.16486400365829468, 0.16486400365829468, 0.1658879965543747]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.14028799533843994, 0.14028799533843994, 0.14131200313568115]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.1525759994983673, 0.1515520066022873, 0.1525759994983673]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.2017280012369156, 0.20070399343967438, 0.2027519941329956]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.17715199291706085, 0.17612800002098083, 0.17983999848365784]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.2662400007247925, 0.2661759853363037, 0.2697215974330902]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.35225600004196167, 0.3521408140659332, 0.3532800078392029]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.1443839967250824, 0.14336000382900238, 0.14528000354766846]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.1658879965543747, 0.1658879965543747, 0.1669120043516159]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.17510400712490082, 0.1740799993276596, 0.17713280022144318]\n",
      "Autotuning results for function merged_qlora_forward_kernel (3072, 2048, 2048, 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.48019200563430786, 0.4628480076789856, 0.4843519926071167]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.35737600922584534, 0.3543039858341217, 0.3643392026424408]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.4065279960632324, 0.40038400888442993, 0.41471999883651733]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.5232639908790588, 0.5191680192947388, 0.5334848165512085]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.49663999676704407, 0.48947200179100037, 0.5099520087242126]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.7884799838066101, 0.7813119888305664, 0.8142848014831543]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [1.0158079862594604, 1.0127359628677368, 1.0169408321380615]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.375216007232666, 0.37068799138069153, 0.38031360507011414]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.42905598878860474, 0.4259839951992035, 0.4341759979724884]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.48742398619651794, 0.4833280146121979, 0.4904960095882416]\n",
      "Autotuning results for function merged_qlora_forward_kernel (5120, 2048, 2048, 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.7884799838066101, 0.7833600044250488, 0.7954175472259521]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.5816320180892944, 0.5754879713058472, 0.5857279896736145]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.6707199811935425, 0.66457599401474, 0.6789119839668274]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.857088029384613, 0.8498560190200806, 0.8775680065155029]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.834559977054596, 0.8273919820785522, 0.8523775935173035]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [1.3281279802322388, 1.3137919902801514, 1.358847975730896]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [1.6752640008926392, 1.674239993095398, 1.6805888414382935]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.6113280057907104, 0.6010879874229431, 0.6178815960884094]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.6973440051078796, 0.6922240257263184, 0.707584023475647]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.8232960104942322, 0.803827166557312, 0.8263487815856934]\n",
      "Autotuning results for function merged_qlora_forward_kernel (7168, 2048, 2048, 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [1.099776029586792, 1.0926079750061035, 1.1106687784194946]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.8161280155181885, 0.8089600205421448, 0.8294399976730347]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.9318400025367737, 0.9226239919662476, 0.9431040287017822]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [1.1673599481582642, 1.1530239582061768, 1.1841535568237305]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [1.1704319715499878, 1.1612160205841064, 1.1851775646209717]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [1.8120160102844238, 1.8044543266296387, 1.827839970588684]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [2.2415359020233154, 2.238464117050171, 2.2773759365081787]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.8191999793052673, 0.8089600205421448, 0.8304640054702759]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.9503040313720703, 0.9410560131072998, 0.9625599980354309]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [1.1489280462265015, 1.1407359838485718, 1.1606016159057617]\n",
      "Autotuning results for function merged_qlora_forward_kernel (9216, 2048, 2048, 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [1.4520319700241089, 1.4469120502471924, 1.4592000246047974]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [1.0516480207443237, 1.0342400074005127, 1.0618879795074463]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [1.2328959703445435, 1.2216320037841797, 1.23678719997406]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [1.5544320344924927, 1.5421439409255981, 1.5800319910049438]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [1.5083200931549072, 1.4955263137817383, 1.5283839702606201]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [2.421247959136963, 2.4043519496917725, 2.4424448013305664]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [3.008416175842285, 3.006873607635498, 3.027148723602295]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [1.119744062423706, 1.1143168210983276, 1.1509759426116943]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [1.257472038269043, 1.252351999282837, 1.2707840204238892]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [1.4735360145568848, 1.4714879989624023, 1.4864383935928345]\n",
      "Autotuning results for function merged_qlora_forward_kernel (11264, 2048, 2048, 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [1.7479840517044067, 1.7367360591888428, 1.7551360130310059]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [1.2902400493621826, 1.277951955795288, 1.3015040159225464]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [1.4812159538269043, 1.4561280012130737, 1.4969919919967651]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [1.8385920524597168, 1.8216960430145264, 1.8565119504928589]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [1.8524160385131836, 1.8304448127746582, 1.8718719482421875]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [2.8825600147247314, 2.8618240356445312, 2.9025216102600098]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [3.5379199981689453, 3.511296033859253, 3.5434625148773193]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [1.3056000471115112, 1.2963839769363403, 1.3416447639465332]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [1.5063040256500244, 1.4921728372573853, 1.5222784280776978]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [1.819648027420044, 1.8063360452651978, 1.8339840173721313]\n",
      "Autotuning results for function merged_qlora_forward_kernel (13312, 2048, 2048, 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [2.1094400882720947, 2.102067232131958, 2.116607904434204]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [1.5144959688186646, 1.5125248432159424, 1.5267839431762695]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [1.7694720029830933, 1.751039981842041, 1.790771245956421]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [2.240511894226074, 2.2220799922943115, 2.2609920501708984]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [2.1923840045928955, 2.174976110458374, 2.214707136154175]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [3.499472141265869, 3.4924542903900146, 3.536895990371704]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [4.346367835998535, 4.3438591957092285, 4.396851062774658]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [1.628159999847412, 1.6103423833847046, 1.6775168180465698]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [1.8216960430145264, 1.809510350227356, 1.8462719917297363]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [2.1524479389190674, 2.135040044784546, 2.169856071472168]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>name</th>\n",
       "      <th>baseline</th>\n",
       "      <th>merged_qlora_forward_v1</th>\n",
       "      <th>merged_qlora_forward_v2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>batch_size</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>0.124928</td>\n",
       "      <td>0.147456</td>\n",
       "      <td>0.141312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3072</th>\n",
       "      <td>0.314368</td>\n",
       "      <td>0.350208</td>\n",
       "      <td>0.355328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5120</th>\n",
       "      <td>0.501248</td>\n",
       "      <td>0.567264</td>\n",
       "      <td>0.584704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7168</th>\n",
       "      <td>0.712704</td>\n",
       "      <td>0.783360</td>\n",
       "      <td>0.816128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9216</th>\n",
       "      <td>0.870400</td>\n",
       "      <td>1.005568</td>\n",
       "      <td>1.052672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11264</th>\n",
       "      <td>1.095616</td>\n",
       "      <td>1.235968</td>\n",
       "      <td>1.276928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13312</th>\n",
       "      <td>1.253376</td>\n",
       "      <td>1.458240</td>\n",
       "      <td>1.517568</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "name        baseline  merged_qlora_forward_v1  merged_qlora_forward_v2\n",
       "batch_size                                                            \n",
       "1024        0.124928                 0.147456                 0.141312\n",
       "3072        0.314368                 0.350208                 0.355328\n",
       "5120        0.501248                 0.567264                 0.584704\n",
       "7168        0.712704                 0.783360                 0.816128\n",
       "9216        0.870400                 1.005568                 1.052672\n",
       "11264       1.095616                 1.235968                 1.276928\n",
       "13312       1.253376                 1.458240                 1.517568"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "params_grid = {\n",
    "    \"batch_size\": [1024 * i for i in range(1, 15, 2)],\n",
    "    \"hidden_size\": [2048],\n",
    "    \"rank\": [16]\n",
    "}\n",
    "bench_input_list = profile.params_grid_to_list(params_grid)\n",
    "batch_size_result = profile.benchmark(only_my_impl, bench_input_list)\n",
    "\n",
    "pivoted = batch_size_result.pivot(index=\"batch_size\", columns=\"name\", values=\"ms\")\n",
    "pivoted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'only_my_impl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m params_grid \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m*\u001b[39m i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m2\u001b[39m)],\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m1536\u001b[39m],\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m16\u001b[39m]\n\u001b[1;32m      5\u001b[0m }\n\u001b[1;32m      7\u001b[0m bench_input_list \u001b[38;5;241m=\u001b[39m profile\u001b[38;5;241m.\u001b[39mparams_grid_to_list(params_grid)\n\u001b[0;32m----> 9\u001b[0m batch_size_result \u001b[38;5;241m=\u001b[39m profile\u001b[38;5;241m.\u001b[39mbenchmark( \u001b[43monly_my_impl\u001b[49m, bench_input_list)\n\u001b[1;32m     11\u001b[0m pivoted \u001b[38;5;241m=\u001b[39m batch_size_result\u001b[38;5;241m.\u001b[39mpivot(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m, columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m, values\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m pivoted\n",
      "\u001b[0;31mNameError\u001b[0m: name 'only_my_impl' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "params_grid = {\n",
    "    \"batch_size\": [1024 * i for i in range(1, 15, 2)],\n",
    "    \"hidden_size\": [1536],\n",
    "    \"rank\": [16]\n",
    "}\n",
    "\n",
    "bench_input_list = profile.params_grid_to_list(params_grid)\n",
    "\n",
    "batch_size_result = profile.benchmark( only_my_impl, bench_input_list)\n",
    "\n",
    "pivoted = batch_size_result.pivot(index=\"batch_size\", columns=\"name\", values=\"ms\")\n",
    "\n",
    "pivoted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autotuning results for function merged_qlora_forward_kernel (1024, 2048, 2048, 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.13926400244235992, 0.13926400244235992, 0.14028799533843994]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.1372160017490387, 0.1372160017490387, 0.1372160017490387]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.14745600521564484, 0.14643199741840363, 0.1476864069700241]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.15052799880504608, 0.14950400590896606, 0.15052799880504608]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.17612800002098083, 0.1740799993276596, 0.17817600071430206]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.21196800470352173, 0.21084800362586975, 0.2181120067834854]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.22220799326896667, 0.22118400037288666, 0.2242559939622879]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.14233599603176117, 0.14131200313568115, 0.14233599603176117]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.13926400244235992, 0.13926400244235992, 0.13934719562530518]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.17392000555992126, 0.1730560064315796, 0.1740799993276596]\n",
      "Autotuning results for function merged_qlora_forward_kernel (3072, 2048, 2048, 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.3553279936313629, 0.35225600004196167, 0.3604480028152466]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.3491840064525604, 0.3461120128631592, 0.35553282499313354]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.37785598635673523, 0.37376001477241516, 0.3850240111351013]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.38707199692726135, 0.38092800974845886, 0.39423999190330505]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.5012480020523071, 0.4935680031776428, 0.5242879986763]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.605184018611908, 0.5990399718284607, 0.6232064366340637]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [0.6164479851722717, 0.6113280057907104, 0.6299647688865662]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.3563520014286041, 0.3543039858341217, 0.3583679795265198]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.3543039858341217, 0.35123199224472046, 0.3612031936645508]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.4904960095882416, 0.4843519926071167, 0.5009664297103882]\n",
      "Autotuning results for function merged_qlora_forward_kernel (5120, 2048, 2048, 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.5877760052680969, 0.5847039818763733, 0.596992015838623]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.562175989151001, 0.5570560097694397, 0.5765119791030884]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.6174079775810242, 0.6123520135879517, 0.6184960007667542]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.6287360191345215, 0.6236159801483154, 0.6440959572792053]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.8335360288619995, 0.8204287886619568, 0.8507391810417175]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [1.022976040840149, 1.0140416622161865, 1.0360832214355469]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [1.0373120307922363, 1.0321919918060303, 1.0455039739608765]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.5949440002441406, 0.5898240208625793, 0.6010879874229431]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.5903360247612, 0.5836799740791321, 0.6010879874229431]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.808463990688324, 0.8017920255661011, 0.8259583711624146]\n",
      "Autotuning results for function merged_qlora_forward_kernel (7168, 2048, 2048, 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.8314880132675171, 0.8202239871025085, 0.8419327735900879]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.7905279994010925, 0.7833600044250488, 0.8038399815559387]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.8611359596252441, 0.8529919981956482, 0.8683519959449768]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.8831999897956848, 0.8755199909210205, 0.8918784260749817]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [1.1699199676513672, 1.1519999504089355, 1.1816960573196411]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [1.4366719722747803, 1.4346239566802979, 1.4611456394195557]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [1.4561280012130737, 1.4540799856185913, 1.4673919677734375]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.8325120210647583, 0.826367974281311, 0.8423423767089844]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [0.8180959820747375, 0.8161280155181885, 0.8314880132675171]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [1.1407359838485718, 1.1376639604568481, 1.1546623706817627]\n",
      "Autotuning results for function merged_qlora_forward_kernel (9216, 2048, 2048, 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [1.0853760242462158, 1.070080041885376, 1.1028480529785156]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [1.0199040174484253, 1.0106879472732544, 1.0424319505691528]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [1.112064003944397, 1.1018240451812744, 1.1233279705047607]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [1.1325440406799316, 1.1315200328826904, 1.1519999504089355]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [1.5001599788665771, 1.487615942955017, 1.5161343812942505]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [1.8580479621887207, 1.844223976135254, 1.8892799615859985]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [1.884160041809082, 1.8831104040145874, 1.898368000984192]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [1.0659840106964111, 1.0567679405212402, 1.0762239694595337]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [1.0547200441360474, 1.0427391529083252, 1.0720512866973877]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [1.4643199443817139, 1.4592000246047974, 1.4794752597808838]\n",
      "Autotuning results for function merged_qlora_forward_kernel (11264, 2048, 2048, 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [1.333184003829956, 1.3156352043151855, 1.3424639701843262]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [1.2380160093307495, 1.2380160093307495, 1.2554240226745605]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [1.3608959913253784, 1.3486080169677734, 1.3660160303115845]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [1.386031985282898, 1.3742079734802246, 1.399603247642517]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [1.842687964439392, 1.8288639783859253, 1.8565119504928589]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [2.274303913116455, 2.251980781555176, 2.2947585582733154]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [2.2906880378723145, 2.288640022277832, 2.30993914604187]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [1.3056000471115112, 1.294335961341858, 1.3178880214691162]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [1.3081599473953247, 1.2853248119354248, 1.312563180923462]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [1.7909760475158691, 1.7817599773406982, 1.8051072359085083]\n",
      "Autotuning results for function merged_qlora_forward_kernel (13312, 2048, 2048, 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16', 'torch.bfloat16'):\n",
      "config: block_M: 128, block_N: 256, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [1.5697920322418213, 1.542579174041748, 1.5800319910049438]\n",
      "config: block_M: 128, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [1.4796799421310425, 1.4704639911651611, 1.4940160512924194]\n",
      "config: block_M: 128, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [1.5994880199432373, 1.5994880199432373, 1.6158720254898071]\n",
      "config: block_M: 64, block_N: 128, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [1.6302080154418945, 1.6291840076446533, 1.6519936323165894]\n",
      "config: block_M: 128, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [2.1928958892822266, 2.181734323501587, 2.2007808685302734]\n",
      "config: block_M: 64, block_N: 32, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [2.7105278968811035, 2.7054080963134766, 2.732032060623169]\n",
      "config: block_M: 32, block_N: 64, block_K: 32, R: 16, GROUP_SIZE_M: 8, num_warps: 2, num_ctas: 1, num_stages: 5, maxnreg: None, time: [2.7443199157714844, 2.7197439670562744, 2.7699201107025146]\n",
      "config: block_M: 128, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 256, block_N: 64, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 64, block_N: 256, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 128, block_K: 128, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [inf, inf, inf]\n",
      "config: block_M: 128, block_N: 64, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [1.551360011100769, 1.539072036743164, 1.5677440166473389]\n",
      "config: block_M: 64, block_N: 128, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [1.5340479612350464, 1.5319039821624756, 1.5532032251358032]\n",
      "config: block_M: 128, block_N: 32, block_K: 64, R: 16, GROUP_SIZE_M: 8, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None, time: [2.1432321071624756, 2.126847982406616, 2.1493759155273438]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>name</th>\n",
       "      <th>merged_qlora_forward_v1</th>\n",
       "      <th>qlora_addition_part</th>\n",
       "      <th>qlora_dense_part</th>\n",
       "      <th>qlora_forward</th>\n",
       "      <th>qlora_low_rank_part</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>batch_size</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>0.138240</td>\n",
       "      <td>0.021504</td>\n",
       "      <td>0.076800</td>\n",
       "      <td>0.122880</td>\n",
       "      <td>0.023552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3072</th>\n",
       "      <td>0.346112</td>\n",
       "      <td>0.057344</td>\n",
       "      <td>0.213024</td>\n",
       "      <td>0.317440</td>\n",
       "      <td>0.050176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5120</th>\n",
       "      <td>0.572416</td>\n",
       "      <td>0.092160</td>\n",
       "      <td>0.354304</td>\n",
       "      <td>0.508928</td>\n",
       "      <td>0.073728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7168</th>\n",
       "      <td>0.796672</td>\n",
       "      <td>0.126976</td>\n",
       "      <td>0.504928</td>\n",
       "      <td>0.717824</td>\n",
       "      <td>0.107520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9216</th>\n",
       "      <td>1.021952</td>\n",
       "      <td>0.161792</td>\n",
       "      <td>0.612352</td>\n",
       "      <td>0.878592</td>\n",
       "      <td>0.130048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11264</th>\n",
       "      <td>1.251328</td>\n",
       "      <td>0.196608</td>\n",
       "      <td>0.782336</td>\n",
       "      <td>1.101824</td>\n",
       "      <td>0.159744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13312</th>\n",
       "      <td>1.471968</td>\n",
       "      <td>0.231536</td>\n",
       "      <td>0.897024</td>\n",
       "      <td>1.265152</td>\n",
       "      <td>0.182272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "name        merged_qlora_forward_v1  qlora_addition_part  qlora_dense_part  \\\n",
       "batch_size                                                                   \n",
       "1024                       0.138240             0.021504          0.076800   \n",
       "3072                       0.346112             0.057344          0.213024   \n",
       "5120                       0.572416             0.092160          0.354304   \n",
       "7168                       0.796672             0.126976          0.504928   \n",
       "9216                       1.021952             0.161792          0.612352   \n",
       "11264                      1.251328             0.196608          0.782336   \n",
       "13312                      1.471968             0.231536          0.897024   \n",
       "\n",
       "name        qlora_forward  qlora_low_rank_part  \n",
       "batch_size                                      \n",
       "1024             0.122880             0.023552  \n",
       "3072             0.317440             0.050176  \n",
       "5120             0.508928             0.073728  \n",
       "7168             0.717824             0.107520  \n",
       "9216             0.878592             0.130048  \n",
       "11264            1.101824             0.159744  \n",
       "13312            1.265152             0.182272  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "params_grid = {\n",
    "    \"batch_size\": [1024 * i for i in range(1, 15, 2)],\n",
    "    \"hidden_size\": [2048],\n",
    "    \"rank\": [16]\n",
    "}\n",
    "\n",
    "bench_input_list = profile.params_grid_to_list(params_grid)\n",
    "\n",
    "batch_size_result = profile.benchmark(\n",
    "    {\n",
    "        \"qlora_forward\": qlora_forward,\n",
    "        \"qlora_low_rank_part\": qlora_low_rank_part,\n",
    "        \"qlora_dense_part\": qlora_dense_part,\n",
    "        \"qlora_addition_part\": qlora_addition_part,\n",
    "        \"merged_qlora_forward_v1\": merged_qlora_forward_v1\n",
    "    },\n",
    "    bench_input_list\n",
    ")\n",
    "\n",
    "pivoted = batch_size_result.pivot(index=\"batch_size\", columns=\"name\", values=\"ms\")\n",
    "\n",
    "pivoted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
